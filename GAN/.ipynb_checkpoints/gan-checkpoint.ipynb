{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \n",
      "Music21 v.4 is the last version that will support Python 2.\n",
      "Please start using Python 3 instead.\n",
      "\n",
      "Set music21.environment.UserSettings()['warnings'] = 0\n",
      "to disable this message.\n",
      "\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append( '../')\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "('Discriminator loss :', 63.98830530216219, 'Generator loss :', 15.179629388798507)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "BATCH_SIZE = 2**3\n",
    "epochs=1\n",
    "\n",
    "# length of the sequence generated\n",
    "TIMESTEPS=50\n",
    "\n",
    "# files to generate the dataset\n",
    "midi_files = glob.glob('../Sounds_for_training/*.mid')[:2]\n",
    "\n",
    "dataset_string,offset=utils.concat_dataset(midi_files)\n",
    "print(1)\n",
    "dataset_number,unique_dic=utils.create_dataset_number(dataset_string)\n",
    "print(2)\n",
    "dataset_number=np.hstack((dataset_number,offset.reshape(-1,1)))\n",
    "\n",
    "\n",
    "# normalize the dataset\n",
    "mean1=np.mean(dataset_number[:,0])\n",
    "std1=np.std(dataset_number[:,0])\n",
    "dataset_number[:,0]-=mean1\n",
    "dataset_number[:,0]/=std1\n",
    "mean2=np.mean(dataset_number[:,1])\n",
    "std2=np.std(dataset_number[:,1])\n",
    "dataset_number[:,1]-=mean2\n",
    "dataset_number[:,1]/=std2\n",
    "\n",
    "x_train=utils.get_x(dataset_number,TIMESTEPS)\n",
    "print(3)\n",
    "X_G = tf.placeholder(\"float\", [None, x_train.shape[1],1])\n",
    "X_real = tf.placeholder(\"float\", [None, x_train.shape[1],x_train.shape[2]])\n",
    "\n",
    "def noise(size):\n",
    "    return np.random.randn(size,TIMESTEPS,1)\n",
    "\n",
    "def generator(X_G): \n",
    "    with tf.variable_scope('generator',reuse=False):\n",
    "        x = tf.layers.conv1d(inputs=X_G, filters=16, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "        x = tf.layers.conv1d(inputs=x, filters=32, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "        x = tf.layers.conv1d(inputs=x, filters=128, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "        x = tf.layers.conv1d(inputs=x, filters=2, kernel_size=3, padding='same')\n",
    "        return x\n",
    "\n",
    "def discriminator(x,reuse=False):\n",
    "    with tf.variable_scope('discriminator',reuse=reuse):\n",
    "        x = tf.layers.conv1d(inputs=x, filters=32, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "        x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "        x = tf.layers.conv1d(inputs=x, filters=128, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "        x=tf.layers.dense(x,128,activation=tf.nn.elu)\n",
    "        x=tf.layers.dense(x,1)\n",
    "        return x\n",
    "\n",
    "\n",
    "#generate a sequence when giving some random noise\n",
    "output_g=generator(X_G)\n",
    "\n",
    "# should tell it's a fake sequence because we are giving the ouput of the generator\n",
    "output_d_fake=discriminator(output_g)\n",
    "\n",
    "# should tell it's a true sequence because we are giving a true sequence\n",
    "output_d_real=discriminator(X_real,reuse=True)\n",
    "\n",
    "T_vars = tf.trainable_variables()\n",
    "\n",
    "#list to train only the generator layers\n",
    "var_list_G = [var for var in T_vars if var.name.startswith('generator')]\n",
    "\n",
    "#list to train only the discriminator layers\n",
    "var_list_D = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "\n",
    "#the diagonale in the the confusion matrix\n",
    "#compute the error of missclassified image (it says it's false instead of true) \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_d_real, labels=tf.ones_like(output_d_real)))\n",
    "#compute the error of missclassified image (it says it's true instead of false) \n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_d_fake, labels=tf.zeros_like(output_d_fake)))\n",
    "\n",
    "#discriminator's error\n",
    "loss1 = D_loss_real + D_loss_fake\n",
    "\n",
    "#generator's error\n",
    "loss2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_d_fake, labels=tf.ones_like(output_d_fake)))\n",
    "\n",
    "# we train both discrimantor and generator\n",
    "optimizer1 = tf.train.AdamOptimizer(learning_rate).minimize(loss1,var_list=var_list_D)\n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate).minimize(loss2,var_list=var_list_G)\n",
    "\n",
    "z_sample=np.random.rand(1,TIMESTEPS,1)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "val_loss_list = []   \n",
    "for epoch in range(0,epochs):\n",
    "#     we train the discriminator\n",
    "    loss1_count=0\n",
    "    loss2_count=0\n",
    "    for batch_i in range(0, x_train.shape[0], BATCH_SIZE):\n",
    "        _,loss_1 = sess.run([optimizer1,loss1], feed_dict={\n",
    "            X_G: noise(x_train[batch_i:batch_i + BATCH_SIZE].shape[0]),\n",
    "            X_real: x_train[batch_i:batch_i + BATCH_SIZE]})\n",
    "        loss1_count+=loss_1\n",
    "    # we train the generator\n",
    "    for batch_i in range(0, x_train.shape[0], BATCH_SIZE):\n",
    "        _,loss_2 = sess.run([optimizer2,loss2], feed_dict={X_G: noise(BATCH_SIZE)})\n",
    "        loss2_count+=loss_2    \n",
    "    print('Discriminator loss :',loss1_count,'Generator loss :',loss2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.227974    0.16638172]\n",
      "[34.58263     0.15889525]\n",
      "[44.77153     0.33934122]\n",
      "[45.38925     0.32347846]\n",
      "oui\n",
      "[29.06872     0.21631223]\n",
      "oui\n",
      "[35.040817   0.1625028]\n",
      "[47.607983   0.4248582]\n",
      "oui\n",
      "[40.61753     0.23517792]\n",
      "[48.584084    0.41118085]\n",
      "oui\n",
      "[38.36093     0.26770455]\n",
      "[51.67627     0.44438764]\n",
      "[49.188896   0.3144213]\n",
      "[34.347404    0.23737258]\n",
      "[46.006096    0.32732904]\n",
      "[36.30009     0.19922239]\n",
      "[47.04889     0.26603717]\n",
      "[37.600124    0.20318826]\n",
      "[40.26529    0.3374461]\n",
      "[40.04942     0.31681257]\n",
      "[45.112526    0.22763713]\n",
      "[31.171186    0.11333027]\n",
      "[35.306797    0.26980925]\n",
      "[43.595398    0.28715956]\n",
      "[43.44748     0.20180956]\n",
      "[34.42011     0.20524257]\n",
      "[43.958363    0.32682514]\n",
      "[44.472443   0.2722245]\n",
      "[35.127613    0.22124116]\n",
      "[46.897255    0.30583215]\n",
      "[40.40229     0.20125861]\n",
      "[53.29831     0.30765125]\n",
      "[35.194283    0.26004392]\n",
      "oui\n",
      "[38.78119     0.24298127]\n",
      "oui\n",
      "[32.19472     0.19509111]\n",
      "[48.79071     0.44625938]\n",
      "oui\n",
      "[28.522644    0.15475398]\n",
      "[55.082573    0.46286583]\n",
      "[53.1081      0.27636236]\n",
      "[46.539886    0.29476345]\n",
      "[48.026344   0.3969558]\n",
      "oui\n",
      "[36.346306   0.2839204]\n",
      "[45.283867    0.43251902]\n"
     ]
    }
   ],
   "source": [
    "#generate noise for the generator in order to generate a sequence\n",
    "z_sample=np.random.rand(1,TIMESTEPS,1)\n",
    "#generate the sequence with the nn above\n",
    "a=sess.run(output_g,feed_dict={X_G:z_sample})\n",
    "#get the result\n",
    "music_generated=a[0]\n",
    "#unormalize\n",
    "music_generated[:,0]*=std1\n",
    "music_generated[:,0]+=mean1\n",
    "music_generated[:,1]*=std2\n",
    "music_generated[:,1]+=mean2\n",
    "\n",
    "#transform the result to a midi file\n",
    "utils.array_to_midi(music_generated,unique_dic,'test0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
