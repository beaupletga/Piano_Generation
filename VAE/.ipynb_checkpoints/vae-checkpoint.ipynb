{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \n",
      "Music21 v.4 is the last version that will support Python 2.\n",
      "Please start using Python 3 instead.\n",
      "\n",
      "Set music21.environment.UserSettings()['warnings'] = 0\n",
      "to disable this message.\n",
      "\n",
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append( '../')\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "('KL divergence :', 70.82048, 'Reconstruction loss :', 633.84357)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# dimension de l'espace latent\n",
    "z_dim=30\n",
    "#BETA=1 <=> VAE\n",
    "#BETA>1 <=> Beta-VAE\n",
    "# paramètre hyper important et marque l'importance que tu donnes à la KL divergence\n",
    "#je te conseilles de le tuner et de le mettre >200 pour voir le résultat (ca devrait pas trop changer)\n",
    "BETA=1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "BATCH_SIZE = 2**5\n",
    "epochs=1\n",
    "TIMESTEPS=50\n",
    "\n",
    "#midi files for the training\n",
    "midi_files = glob.glob('../Sounds_for_training/*.mid')[:2]\n",
    "\n",
    "\n",
    "dataset_string,offset=utils.concat_dataset(midi_files)\n",
    "print(1)\n",
    "dataset_number,unique_dic=utils.create_dataset_number(dataset_string)\n",
    "print(2)\n",
    "dataset_number=np.hstack((dataset_number,offset.reshape(-1,1)))\n",
    "\n",
    "#normalize the dataset\n",
    "mean1=np.mean(dataset_number[:,0])\n",
    "std1=np.std(dataset_number[:,0])\n",
    "dataset_number[:,0]-=mean1\n",
    "dataset_number[:,0]/=std1\n",
    "mean2=np.mean(dataset_number[:,1])\n",
    "std2=np.std(dataset_number[:,1])\n",
    "dataset_number[:,1]-=mean2\n",
    "dataset_number[:,1]/=std2\n",
    "\n",
    "x_train=utils.get_x(dataset_number,TIMESTEPS)\n",
    "print(3)\n",
    "X = tf.placeholder(\"float\", [None, x_train.shape[1],x_train.shape[2]])\n",
    "\n",
    "\n",
    "def encoder(X):  \n",
    "    x = tf.layers.conv1d(inputs=X, filters=16, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=32, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=128, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    x = tf.layers.dense(x,128,activation=tf.nn.elu)\n",
    "    z_mean = tf.layers.dense(x, z_dim)\n",
    "    z_std=tf.layers.dense(x, z_dim)\n",
    "    return z_mean,z_std\n",
    "\n",
    "def sample_eps(z_mean,z_std):\n",
    "    eps=tf.random_normal(tf.shape(z_mean),mean=0.,stddev=1.)\n",
    "    z=z_mean+tf.exp(z_std/2)*eps\n",
    "    return z\n",
    "\n",
    "def decoder(z):\n",
    "    x = tf.layers.dense(z,TIMESTEPS)\n",
    "    x = tf.reshape(x,[-1,TIMESTEPS,1])\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=64, kernel_size=3, padding='same',activation=tf.nn.elu)\n",
    "    x = tf.layers.conv1d(inputs=x, filters=2, kernel_size=3, padding='same')\n",
    "    x_sigmoid=tf.nn.sigmoid(x)\n",
    "    return x,x_sigmoid\n",
    "\n",
    "#you get the zmean and zstd from the encoder\n",
    "z_mean,z_std=encoder(X)\n",
    "\n",
    "#you generated a normal distribution with the above parameters\n",
    "z=sample_eps(z_mean,z_std)\n",
    "\n",
    "#you get the outuput of the decoder\n",
    "output,output_sigmoid=decoder(z)\n",
    "\n",
    "#the KL divergence\n",
    "cons_loss= 0.5 * tf.reduce_sum(tf.exp(z_std) + z_mean**2 - 1. - z_std, 1)\n",
    "\n",
    "# X_flat= tf.contrib.layers.flatten(X)\n",
    "#how accurate is the sequence (reconstruction loss)\n",
    "recons_loss=tf.reduce_sum(tf.pow(tf.subtract(output, X),2))\n",
    "\n",
    "#the total cost\n",
    "cost = tf.reduce_mean(recons_loss+BETA*cons_loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "val_loss_list1 = []\n",
    "val_loss_list2 = []\n",
    "for epoch in range(epochs):\n",
    "    for batch_i in range(0, x_train.shape[0], BATCH_SIZE):\n",
    "        _,loss1,loss2 = sess.run([optimizer,cons_loss,recons_loss], feed_dict={X: x_train[batch_i:batch_i + BATCH_SIZE]})\n",
    "    print('KL divergence :',np.mean(loss1),'Reconstruction loss :',np.mean(loss2))\n",
    "    val_loss_list1.append(np.mean(loss1))\n",
    "    val_loss_list2.append(np.mean(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.229437   0.4719603]\n",
      "[19.221878   0.5052416]\n",
      "[18.812029    0.41552275]\n",
      "[21.763393    0.36014944]\n",
      "[29.845272   0.4057522]\n",
      "[26.641594    0.52460724]\n",
      "[15.126456    0.50374454]\n",
      "[22.11692     0.37216628]\n",
      "[28.163786   0.4002056]\n",
      "[22.067406    0.51460785]\n",
      "[16.15759    0.4831846]\n",
      "[23.478498   0.3750818]\n",
      "[32.598045    0.37970662]\n",
      "[30.623457    0.49103048]\n",
      "[27.360445   0.5090097]\n",
      "[24.574587    0.43242255]\n",
      "[29.523064   0.3746205]\n",
      "[35.527763   0.4427256]\n",
      "[24.619087   0.5186462]\n",
      "[20.433624    0.42447045]\n",
      "[36.55079    0.3701914]\n",
      "[30.708954    0.49382007]\n",
      "[16.302134   0.4862096]\n",
      "[28.575485   0.3550052]\n",
      "[33.70575     0.41254994]\n",
      "[22.840103   0.5472181]\n",
      "[16.127728   0.4824698]\n",
      "[24.659472    0.37360394]\n",
      "[29.095861   0.3987469]\n",
      "[26.542603    0.47349122]\n",
      "[29.617695    0.47612032]\n",
      "[27.2905     0.4581874]\n",
      "[25.581549   0.4206904]\n",
      "[29.093775    0.40402874]\n",
      "[26.698135   0.4344398]\n",
      "[23.158876    0.44391167]\n",
      "[24.72708     0.44158298]\n",
      "[21.758518    0.47669682]\n",
      "[13.557128    0.47434893]\n",
      "[19.308786    0.40924612]\n",
      "[23.08252     0.40300733]\n",
      "[22.463303    0.42240936]\n",
      "[34.41793     0.44539505]\n",
      "[29.8753     0.5251677]\n",
      "[18.870821    0.47559527]\n",
      "[33.133522    0.37374663]\n",
      "[32.97611    0.4218598]\n",
      "[26.39265    0.4736968]\n",
      "[34.761196    0.43805784]\n",
      "[31.003736    0.47061265]\n"
     ]
    }
   ],
   "source": [
    "# you take a sample of normal distribution\n",
    "lala=[]\n",
    "for i in range(1):\n",
    "    z_sample=np.random.rand(1,z_dim)\n",
    "    #you run it trough the nn\n",
    "    a=sess.run(output,feed_dict={z:z_sample})\n",
    "    #you get the result\n",
    "    music_generated=a[0]\n",
    "    #unormalize the result\n",
    "    music_generated[:,0]*=std1\n",
    "    music_generated[:,0]+=mean1\n",
    "    music_generated[:,1]*=std2\n",
    "    #add a bit of time to the offset (the time between each note)\n",
    "    music_generated[:,1]+=mean2+0.25\n",
    "    lala.append(music_generated[:,0])\n",
    "    #you transform the array generated by a midi file\n",
    "    utils.array_to_midi(music_generated,unique_dic,name='test'+str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
